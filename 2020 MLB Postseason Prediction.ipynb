{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2020 MLB Postseason Participants Prediction<br>\n",
    "\n",
    "#Introduction<br>\n",
    "\n",
    "2020 MLB season has already been hard to predict given it's short nature. This research aims to build a perfect model using team batting stats of the past eight years and then use the model to predict which teams' batting stats on 08/13/2020 is worthy of getting into postseason on a traditional 10-team postseason format.<br>\n",
    "\n",
    "#Methods<br>\n",
    "\n",
    "Model was built using the combination of 16 team regular season stats:PA, R, H, 2B, 3B, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB from 2012-2019 and whether that corresponding team went into postseason or not.<br>\n",
    "\n",
    "Using SQL Server and Python<br>\n",
    "\n",
    "logistic regression<br>\n",
    "k-nearest neighbors<br>\n",
    "support vector machine<br>\n",
    "decision tree<br>\n",
    "random forest<br>\n",
    "gradient boosting<br>\n",
    "XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 1: Import data<br>\n",
    "\n",
    "import regular season stats from MLB teams who got into postseason during 2012-2019<br>\n",
    "\n",
    "items include Tm, PA, R, H, 2B, 3B, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB<br>\n",
    "\n",
    "total rows are 8(years)*10(teams each year)=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "sql_conn = pyodbc.connect('''DRIVER={ODBC Driver 13 for SQL Server};\n",
    "                            SERVER=ALLENHO\\MSSQLSERVER002;\n",
    "                            DATABASE=Playoffbound;\n",
    "                            Trusted_Connection=yes''') \n",
    "query = '''\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['19B$']\n",
    "where Tm in ('WSN','LAD','MIL','ATL','STL','HOU','NYY','MIN','TBR','OAK')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['18B$']\n",
    "where Tm in ('BOS','LAD','MIL','ATL','CHC','HOU','NYY','CLE','COL','OAK')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['17B$']\n",
    "where Tm in ('BOS','LAD','COL','WSN','CHC','HOU','NYY','CLE','ARI','MIN')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['16B$']\n",
    "where Tm in ('TOR','CLE','BOS','BAL','TEX','NYM','CHC','LAD','WSN','SFG')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['15B$']\n",
    "where Tm in ('TOR','KCR','HOU','NYY','TEX','NYM','CHC','LAD','STL','PIT')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['14B$']\n",
    "where Tm in ('BAL','KCR','OAK','LAA','DET','WSN','STL','LAD','PIT','SFG')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['13B$']\n",
    "where Tm in ('BOS','TBR','OAK','CLE','DET','ATL','STL','LAD','PIT','CIN')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['12B$']\n",
    "where Tm in ('TEX','BAL','OAK','NYY','DET','ATL','STL','SFG','WSN','CIN')\n",
    "'''\n",
    "df = pd.read_sql(query, sql_conn)\n",
    "\n",
    "#stored as df_post\n",
    "df_post = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import regular season stats from MLB teams who DIDN'T get into postseason during 2012-2019<br>\n",
    "items are the same as above<br>\n",
    "total rows are 8(years)*20(teams each year)=160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Tm      PA      R       H     HR    RBI     SB    CS     BB      SO  \\\n",
      "0    ATL  6302.0  855.0  1432.0  249.0  824.0   89.0  28.0  619.0  1467.0   \n",
      "1    HOU  6394.0  920.0  1538.0  288.0  891.0   67.0  27.0  645.0  1166.0   \n",
      "2    LAD  6282.0  886.0  1414.0  279.0  861.0   57.0  10.0  607.0  1356.0   \n",
      "3    MIL  6309.0  769.0  1366.0  250.0  744.0  101.0  25.0  629.0  1563.0   \n",
      "4    MIN  6392.0  939.0  1547.0  307.0  906.0   28.0  21.0  525.0  1334.0   \n",
      "..   ...     ...    ...     ...    ...    ...    ...   ...    ...     ...   \n",
      "155  PIT  6014.0  651.0  1313.0  170.0  620.0   73.0  52.0  444.0  1354.0   \n",
      "156  SDP  6112.0  651.0  1339.0  121.0  610.0  155.0  46.0  539.0  1238.0   \n",
      "157  SEA  6057.0  619.0  1285.0  149.0  584.0  104.0  35.0  466.0  1259.0   \n",
      "158  TBR  6105.0  697.0  1293.0  175.0  665.0  134.0  44.0  571.0  1323.0   \n",
      "159  TOR  6094.0  716.0  1346.0  198.0  677.0  123.0  41.0  473.0  1251.0   \n",
      "\n",
      "        BA    OBP    SLG    OPS      TB  POST  \n",
      "0    0.258  0.336  0.452  0.789  2514.0     1  \n",
      "1    0.274  0.352  0.495  0.848  2781.0     1  \n",
      "2    0.257  0.338  0.472  0.810  2593.0     1  \n",
      "3    0.246  0.329  0.438  0.767  2429.0     1  \n",
      "4    0.270  0.338  0.494  0.832  2832.0     1  \n",
      "..     ...    ...    ...    ...     ...   ...  \n",
      "155  0.243  0.304  0.395  0.699  2138.0     0  \n",
      "156  0.247  0.319  0.380  0.699  2060.0     0  \n",
      "157  0.234  0.296  0.369  0.665  2027.0     0  \n",
      "158  0.240  0.317  0.394  0.711  2128.0     0  \n",
      "159  0.245  0.309  0.407  0.716  2231.0     0  \n",
      "\n",
      "[240 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "sql_conn = pyodbc.connect('''DRIVER={ODBC Driver 13 for SQL Server};\n",
    "                            SERVER=ALLENHO\\MSSQLSERVER002;\n",
    "                            DATABASE=Playoffbound;\n",
    "                            Trusted_Connection=yes''') \n",
    "query = '''\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['19B$']\n",
    "where Tm is not null and Tm not in ('WSN','LAD','MIL','ATL','STL','HOU','NYY','MIN','TBR','OAK', 'LgAvg')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['18B$']\n",
    "where Tm is not null and Tm not in ('BOS','LAD','MIL','ATL','CHC','HOU','NYY','CLE','COL','OAK', 'LgAvg')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['17B$']\n",
    "where Tm is not null and Tm not in ('BOS','LAD','COL','WSN','CHC','HOU','NYY','CLE','ARI','MIN', 'LgAvg')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['16B$']\n",
    "where Tm is not null and Tm not in ('TOR','CLE','BOS','BAL','TEX','NYM','CHC','LAD','WSN','SFG', 'LgAvg')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['15B$']\n",
    "where Tm is not null and Tm not in ('TOR','KCR','HOU','NYY','TEX','NYM','CHC','LAD','STL','PIT', 'LgAvg')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['14B$']\n",
    "where Tm is not null and Tm not in ('BAL','KCR','OAK','LAA','DET','WSN','STL','LAD','PIT','SFG', 'LgAvg')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['13B$']\n",
    "where Tm is not null and Tm not in ('BOS','TBR','OAK','CLE','DET','ATL','STL','LAD','PIT','CIN', 'LgAvg')\n",
    "UNION ALL\n",
    "select Tm, PA, R, H, HR, RBI, SB, CS, BB, SO, BA, OBP, SLG, OPS, TB\n",
    "from [dbo].['12B$']\n",
    "where Tm is not null and Tm not in ('TEX','BAL','OAK','NYY','DET','ATL','STL','SFG','WSN','CIN', 'LgAvg')\n",
    "'''\n",
    "df = pd.read_sql(query, sql_conn)\n",
    "\n",
    "#stored as df_npost\n",
    "df_npost = df\n",
    "\n",
    "#add each dataframe a new column named POST, which imply whether the team made the postseason that year\n",
    "df_post['POST']= 1\n",
    "df_npost['POST']= 0\n",
    "\n",
    "#append two dataframes together\n",
    "df_com=df_post.append(df_npost)\n",
    "\n",
    "#take a look at the table we got\n",
    "print(df_com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a brief look at the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                PA           R            H          HR         RBI  \\\n",
      "count   240.000000  240.000000   240.000000  240.000000  240.000000   \n",
      "mean   6158.929167  712.929167  1397.520833  178.191667  679.350000   \n",
      "std      97.271646   77.051138    71.511707   40.684477   75.512603   \n",
      "min    5905.000000  513.000000  1199.000000   95.000000  485.000000   \n",
      "25%    6085.250000  652.500000  1346.000000  148.000000  622.500000   \n",
      "50%    6154.500000  709.500000  1390.500000  174.500000  675.000000   \n",
      "75%    6224.250000  761.000000  1446.000000  211.000000  728.000000   \n",
      "max    6475.000000  943.000000  1625.000000  307.000000  906.000000   \n",
      "\n",
      "              SB          CS          BB           SO          BA         OBP  \\\n",
      "count  240.00000  240.000000  240.000000   240.000000  240.000000  240.000000   \n",
      "mean    87.53750   33.195833  499.750000  1296.412500    0.252933    0.319117   \n",
      "std     28.46018    8.737884   63.716652   130.141413    0.010588    0.011853   \n",
      "min     19.00000   10.000000  375.000000   973.000000    0.226000    0.292000   \n",
      "25%     66.00000   27.000000  452.000000  1204.000000    0.245000    0.311000   \n",
      "50%     86.00000   33.000000  500.000000  1290.500000    0.252000    0.319000   \n",
      "75%    105.25000   38.250000  545.500000  1384.500000    0.260000    0.327000   \n",
      "max    181.00000   61.000000  656.000000  1595.000000    0.283000    0.352000   \n",
      "\n",
      "              SLG         OPS           TB        POST  \n",
      "count  240.000000  240.000000   240.000000  240.000000  \n",
      "mean     0.409925    0.729004  2264.937500    0.333333  \n",
      "std      0.026815    0.036499   163.359296    0.472390  \n",
      "min      0.335000    0.627000  1810.000000    0.000000  \n",
      "25%      0.391000    0.702750  2152.500000    0.000000  \n",
      "50%      0.409000    0.728000  2256.500000    0.000000  \n",
      "75%      0.428250    0.752000  2364.000000    1.000000  \n",
      "max      0.495000    0.848000  2832.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(df_com.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a brief look at the correlation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            PA         R         H        HR       RBI        SB        CS  \\\n",
      "PA    1.000000  0.728568  0.603097  0.404630  0.724551 -0.075825 -0.275590   \n",
      "R     0.728568  1.000000  0.627253  0.757987  0.996604 -0.077307 -0.286587   \n",
      "H     0.603097  0.627253  1.000000  0.168541  0.622822  0.010873 -0.096667   \n",
      "HR    0.404630  0.757987  0.168541  1.000000  0.773527 -0.226804 -0.338698   \n",
      "RBI   0.724551  0.996604  0.622822  0.773527  1.000000 -0.101979 -0.300592   \n",
      "SB   -0.075825 -0.077307  0.010873 -0.226804 -0.101979  1.000000  0.563940   \n",
      "CS   -0.275590 -0.286587 -0.096667 -0.338698 -0.300592  0.563940  1.000000   \n",
      "BB    0.672470  0.572770  0.031033  0.490024  0.570726 -0.060325 -0.240242   \n",
      "SO    0.004639  0.060778 -0.417156  0.423937  0.068943 -0.128711 -0.030335   \n",
      "BA    0.512469  0.614537  0.978840  0.149331  0.609907  0.044830 -0.045988   \n",
      "OBP   0.790239  0.828347  0.741023  0.438346  0.823876 -0.004032 -0.162219   \n",
      "SLG   0.582243  0.922851  0.598798  0.864843  0.931042 -0.141123 -0.270421   \n",
      "OPS   0.685823  0.948734  0.681851  0.779146  0.953297 -0.105815 -0.251054   \n",
      "TB    0.658199  0.926030  0.685324  0.816982  0.933121 -0.148223 -0.286691   \n",
      "POST  0.461904  0.488744  0.339784  0.281205  0.481147 -0.079983 -0.224696   \n",
      "\n",
      "            BB        SO        BA       OBP       SLG       OPS        TB  \\\n",
      "PA    0.672470  0.004639  0.512469  0.790239  0.582243  0.685823  0.658199   \n",
      "R     0.572770  0.060778  0.614537  0.828347  0.922851  0.948734  0.926030   \n",
      "H     0.031033 -0.417156  0.978840  0.741023  0.598798  0.681851  0.685324   \n",
      "HR    0.490024  0.423937  0.149331  0.438346  0.864843  0.779146  0.816982   \n",
      "RBI   0.570726  0.068943  0.609907  0.823876  0.931042  0.953297  0.933121   \n",
      "SB   -0.060325 -0.128711  0.044830 -0.004032 -0.141123 -0.105815 -0.148223   \n",
      "CS   -0.240242 -0.030335 -0.045988 -0.162219 -0.270421 -0.251054 -0.286691   \n",
      "BB    1.000000  0.236172  0.035724  0.647519  0.440775  0.534829  0.400129   \n",
      "SO    0.236172  1.000000 -0.463401 -0.188190  0.143080  0.044561  0.101463   \n",
      "BA    0.035724 -0.463401  1.000000  0.769802  0.595888  0.688866  0.655888   \n",
      "OBP   0.647519 -0.188190  0.769802  1.000000  0.735855  0.866479  0.746457   \n",
      "SLG   0.440775  0.143080  0.595888  0.735855  1.000000  0.975517  0.985919   \n",
      "OPS   0.534829  0.044561  0.688866  0.866479  0.975517  1.000000  0.968787   \n",
      "TB    0.400129  0.101463  0.655888  0.746457  0.985919  0.968787  1.000000   \n",
      "POST  0.431907 -0.105900  0.356648  0.546741  0.405289  0.477010  0.403341   \n",
      "\n",
      "          POST  \n",
      "PA    0.461904  \n",
      "R     0.488744  \n",
      "H     0.339784  \n",
      "HR    0.281205  \n",
      "RBI   0.481147  \n",
      "SB   -0.079983  \n",
      "CS   -0.224696  \n",
      "BB    0.431907  \n",
      "SO   -0.105900  \n",
      "BA    0.356648  \n",
      "OBP   0.546741  \n",
      "SLG   0.405289  \n",
      "OPS   0.477010  \n",
      "TB    0.403341  \n",
      "POST  1.000000  \n"
     ]
    }
   ],
   "source": [
    "df_corr = df_com.corr()\n",
    "print(df_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 2: Train the Models\n",
    "\n",
    "There's one other consideration worth making -- the distribution of outcomes is somewhat imbalanced. Teams getting into postseason each year are less than those not. I tried an oversampling technique to see how it affected the models. Oversampling techniques are usually applied to datasets where an outcome is significantly less common. That might be a little bit of a stretch for this scenario, but I think it's worth at least checking if an oversampling technique would help. I tried fitting my different models twice -- with and without oversampling. For oversampling, I used SMOTE (synthetic minority oversampling technique).<br>\n",
    "\n",
    "use 4 metrics to evaluate the models, which together should give a good picture of the best overall model:<br>\n",
    "\n",
    "F1 score (weighted by instances of each label)\n",
    "ROC AUC (computed by label and weighted by frequency)\n",
    "balanced accuracy (for imbalanced datasets)\n",
    "log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split data into X and y\n",
    "X = df_com.loc[:,'PA':'TB']\n",
    "Y = df_com.loc[:,'POST']\n",
    "\n",
    "# scale and center numeric columns\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import f1_score, accuracy_score, log_loss, roc_auc_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# scoring metrics\n",
    "scoring = {\n",
    "    'f1_weighted': 'f1_weighted',\n",
    "    'accuracy': 'balanced_accuracy',\n",
    "    'roc_auc': 'roc_auc_ovr_weighted',\n",
    "    'neg_log_loss': 'neg_log_loss'\n",
    "    }\n",
    "\n",
    "# for results df\n",
    "eval_cols = [\n",
    "    'models',\n",
    "    'F1 Score',\n",
    "    'Balanced Accuracy',\n",
    "    'ROC AUC',\n",
    "    'Neg Log Loss'\n",
    "    ]\n",
    "\n",
    "# define classifier models\n",
    "classifiers = [\n",
    "    LogisticRegression(multi_class='multinomial', max_iter=10000),\n",
    "    KNeighborsClassifier(),\n",
    "    SVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    XGBClassifier()\n",
    "    ]\n",
    "\n",
    "# classifier names\n",
    "clf_names = [\n",
    "    'Logistic Regression',\n",
    "    'KNN',\n",
    "    'SVM',\n",
    "    'Decision Tree',\n",
    "    'Random Forest',\n",
    "    'Gradient Boosting',\n",
    "    'XGBClassifier'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to cross-validate Logistic Regression = 0.009 min.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to cross-validate KNN = 0.003 min.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to cross-validate SVM = 0.006 min.\n",
      "Time to cross-validate Decision Tree = 0.002 min.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to cross-validate Random Forest = 0.061 min.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to cross-validate Gradient Boosting = 0.029 min.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to cross-validate XGBClassifier = 0.015 min.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Neg Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.727067</td>\n",
       "      <td>0.728125</td>\n",
       "      <td>0.826953</td>\n",
       "      <td>-0.533989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.653692</td>\n",
       "      <td>0.659375</td>\n",
       "      <td>0.724414</td>\n",
       "      <td>-3.600277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.624776</td>\n",
       "      <td>0.618750</td>\n",
       "      <td>0.739844</td>\n",
       "      <td>-0.619171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.651017</td>\n",
       "      <td>0.621875</td>\n",
       "      <td>0.621875</td>\n",
       "      <td>-11.800892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.730019</td>\n",
       "      <td>0.706250</td>\n",
       "      <td>0.798828</td>\n",
       "      <td>-0.539176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.706371</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.773828</td>\n",
       "      <td>-0.672079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.712687</td>\n",
       "      <td>0.709375</td>\n",
       "      <td>0.775781</td>\n",
       "      <td>-0.654405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                models  F1 Score  Balanced Accuracy   ROC AUC  Neg Log Loss\n",
       "0  Logistic Regression  0.727067           0.728125  0.826953     -0.533989\n",
       "1                  KNN  0.653692           0.659375  0.724414     -3.600277\n",
       "2                  SVM  0.624776           0.618750  0.739844     -0.619171\n",
       "3        Decision Tree  0.651017           0.621875  0.621875    -11.800892\n",
       "4        Random Forest  0.730019           0.706250  0.798828     -0.539176\n",
       "5    Gradient Boosting  0.706371           0.687500  0.773828     -0.672079\n",
       "6        XGBClassifier  0.712687           0.709375  0.775781     -0.654405"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import time as time\n",
    "import numpy as np\n",
    "\n",
    "f1, acc, roc_auc, log_loss = [], [], [], []\n",
    "for clf, clf_nm in zip(classifiers, clf_names):\n",
    "    \n",
    "    # setup pipeline to oversample, then fit model\n",
    "    pipe = Pipeline([\n",
    "        ('smote', SMOTE()),\n",
    "        ('classify', clf)\n",
    "        ])\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    # cross-validate 5 times\n",
    "    res_smote = cross_validate(pipe, X, Y, cv=5, scoring=scoring)\n",
    "    results_smote = pd.DataFrame(res_smote)\n",
    "    \n",
    "    stop = time.time()\n",
    "\n",
    "    print('Time to cross-validate %s = %0.3f min.' % (clf_nm, (stop - start) / 60))\n",
    "\n",
    "    # save average scores\n",
    "    f1.append(np.mean(results_smote.test_f1_weighted))\n",
    "    acc.append(np.mean(results_smote.test_accuracy))\n",
    "    roc_auc.append(np.mean(results_smote.test_roc_auc))\n",
    "    log_loss.append(np.mean(results_smote.test_neg_log_loss))\n",
    "\n",
    "# save results to df\n",
    "model_eval_smote = pd.DataFrame(data=zip(clf_names, f1, acc, roc_auc, log_loss),\n",
    "                          columns=eval_cols)\n",
    "\n",
    "display(model_eval_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to cross-validate Logistic Regression = 0.002 min.\n",
      "Time to cross-validate KNN = 0.001 min.\n",
      "Time to cross-validate SVM = 0.002 min.\n",
      "Time to cross-validate Decision Tree = 0.001 min.\n",
      "Time to cross-validate Random Forest = 0.021 min.\n",
      "Time to cross-validate Gradient Boosting = 0.015 min.\n",
      "Time to cross-validate XGBClassifier = 0.005 min.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Neg Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.748284</td>\n",
       "      <td>0.715625</td>\n",
       "      <td>0.828516</td>\n",
       "      <td>-0.491554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.668809</td>\n",
       "      <td>0.631250</td>\n",
       "      <td>0.739844</td>\n",
       "      <td>-2.568752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.731454</td>\n",
       "      <td>0.684375</td>\n",
       "      <td>0.751563</td>\n",
       "      <td>-0.546522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.679634</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>-10.793494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.713606</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.808398</td>\n",
       "      <td>-0.537619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.735821</td>\n",
       "      <td>0.706250</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>-0.624170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.734542</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.785547</td>\n",
       "      <td>-0.602285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                models  F1 Score  Balanced Accuracy   ROC AUC  Neg Log Loss\n",
       "0  Logistic Regression  0.748284           0.715625  0.828516     -0.491554\n",
       "1                  KNN  0.668809           0.631250  0.739844     -2.568752\n",
       "2                  SVM  0.731454           0.684375  0.751563     -0.546522\n",
       "3        Decision Tree  0.679634           0.650000  0.650000    -10.793494\n",
       "4        Random Forest  0.713606           0.681250  0.808398     -0.537619\n",
       "5    Gradient Boosting  0.735821           0.706250  0.791016     -0.624170\n",
       "6        XGBClassifier  0.734542           0.703125  0.785547     -0.602285"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time as time\n",
    "f1, acc, roc_auc, log_loss = [], [], [], []\n",
    "for clf, clf_nm in zip(classifiers, clf_names):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    # cross-validate 5 times\n",
    "    res = cross_validate(clf, X, Y, cv=5, scoring=scoring)\n",
    "    results = pd.DataFrame(res)\n",
    "    \n",
    "    stop = time.time()\n",
    "\n",
    "    print('Time to cross-validate %s = %0.3f min.' % (clf_nm, (stop - start) / 60))\n",
    "\n",
    "    # save average scores\n",
    "    f1.append(np.mean(results.test_f1_weighted))\n",
    "    acc.append(np.mean(results.test_accuracy))\n",
    "    roc_auc.append(np.mean(results.test_roc_auc))\n",
    "    log_loss.append(np.mean(results.test_neg_log_loss))\n",
    "\n",
    "# save results to df\n",
    "model_eval = pd.DataFrame(data=zip(clf_names, f1, acc, roc_auc, log_loss),\n",
    "                          columns=eval_cols)\n",
    "\n",
    "display(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like not performing oversampling is the way to go here. The non-SMOTE'd data built models that slightly outperformed the SMOTE'd models. sklearn defines balanced accuracy as the average of recall on each class. Recall only considers false negatives and true positives -- and since SMOTE creates more data to help a model recognize minority classes, it should reduce false negatives. So, it's no surprise the SMOTE'd data performs better in balanced accuracy.<br>\n",
    "\n",
    "Overall, the Logistic Regression model was the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 3: Make Predictions with Logistic Regression model<br>\n",
    "\n",
    "import 2020 team stats as of 08/14/2020 normalized to 162 games, try to see which teams' stats on 08/13/2020 is worthy of getting into postseason on a traditional 10-team postseason format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(multi_class='multinomial')\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20563564,  0.35558267, -0.43472342, -0.07174802, -0.16508224,\n",
       "        -0.01143693, -0.18435707,  0.00724   , -0.18083768,  0.20025544,\n",
       "         0.54154686,  0.04143439,  0.35595693, -0.26669742]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            PA           R            H          HR         RBI         SB  \\\n",
      "0  6096.315789  750.315789  1347.157895  127.894737  707.684211  34.105263   \n",
      "1  5977.800000  842.400000  1312.200000  226.800000  826.200000  64.800000   \n",
      "2  6111.000000  864.000000  1467.000000  243.000000  846.000000  63.000000   \n",
      "3  6096.315789  724.736842  1415.368421  196.105263  682.105263  34.105263   \n",
      "4  6176.250000  840.375000  1296.000000  232.875000  789.750000  50.625000   \n",
      "\n",
      "          CS          BB           SO     BA    OBP    SLG    OPS           TB  \n",
      "0  25.578947  477.473684  1219.263158  0.245  0.315  0.382  0.697  2097.473684  \n",
      "1  24.300000  526.500000  1644.300000  0.244  0.316  0.437  0.753  2349.000000  \n",
      "2  45.000000  513.000000  1305.000000  0.269  0.337  0.473  0.810  2583.000000  \n",
      "3  25.578947  426.315789  1492.105263  0.254  0.316  0.422  0.738  2353.263158  \n",
      "4  10.125000  658.125000  1620.000000  0.244  0.344  0.431  0.776  2288.250000  \n"
     ]
    }
   ],
   "source": [
    "df_2020=pd.read_excel(r'C:\\Users\\allen\\Desktop\\Baseball research\\Postseason or bust\\2020 projection for 0813.xlsx')\n",
    "DF_2020=df_2020.loc[:,['PA162', 'R162', 'H162', 'HR162', 'RBI162', 'SB162', 'CS162', 'BB162', 'SO162', 'BA', 'OBP', 'SLG', 'OPS', 'TB162']]\n",
    "DF_2020['PA']=DF_2020['PA162']\n",
    "DF_2020['R']=DF_2020['R162']\n",
    "DF_2020['H']=DF_2020['H162']\n",
    "DF_2020['HR']=DF_2020['HR162']\n",
    "DF_2020['RBI']=DF_2020['RBI162']\n",
    "DF_2020['SB']=DF_2020['SB162']\n",
    "DF_2020['CS']=DF_2020['CS162']\n",
    "DF_2020['BB']=DF_2020['BB162']\n",
    "DF_2020['SO']=DF_2020['SO162']\n",
    "DF_2020['TB']=DF_2020['TB162']\n",
    "DF_2020=DF_2020.loc[:, ['PA','R','H','HR','RBI','SB','CS','BB','SO','BA','OBP','SLG','OPS','TB']]\n",
    "print(DF_2020.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try SVM model and perform hyperparameter tuning to see if it gives us better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.87      0.80        47\n",
      "           1       0.65      0.44      0.52        25\n",
      "\n",
      "    accuracy                           0.72        72\n",
      "   macro avg       0.70      0.66      0.66        72\n",
      "weighted avg       0.71      0.72      0.71        72\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=0.1, gamma=0.01, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=0.1, gamma=0.01, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=0.1, gamma=0.01, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=0.1, gamma=0.01, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=0.1, gamma=0.01, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=0.1, gamma=0.001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=0.1, gamma=0.001, kernel=rbf, score=0.676, total=   0.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=0.1, gamma=0.001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=0.1, gamma=0.001, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=0.1, gamma=0.001, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=0.1, gamma=0.0001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=0.1, gamma=0.0001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=0.1, gamma=0.0001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=0.1, gamma=0.0001, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=0.1, gamma=0.0001, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.765, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.735, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.735, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.758, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.636, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.853, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.794, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.794, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.788, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.636, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.697, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] ....... C=1, gamma=0.0001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] ....... C=1, gamma=0.0001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] ....... C=1, gamma=0.0001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] ....... C=1, gamma=0.0001, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] ....... C=1, gamma=0.0001, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.647, total=   0.0s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.735, total=   0.0s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.735, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.794, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.618, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.788, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.545, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.882, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.824, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.818, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.636, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.853, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.853, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.794, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.788, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.636, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ...... C=10, gamma=0.0001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ...... C=10, gamma=0.0001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ...... C=10, gamma=0.0001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ...... C=10, gamma=0.0001, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ...... C=10, gamma=0.0001, kernel=rbf, score=0.697, total=   0.0s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.647, total=   0.0s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.735, total=   0.0s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.794, total=   0.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.559, total=   0.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.788, total=   0.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.485, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.824, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.853, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.706, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.848, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.606, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.853, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.853, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.788, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.636, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=100, gamma=0.0001, kernel=rbf, score=0.853, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=100, gamma=0.0001, kernel=rbf, score=0.853, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=100, gamma=0.0001, kernel=rbf, score=0.794, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=100, gamma=0.0001, kernel=rbf, score=0.788, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=100, gamma=0.0001, kernel=rbf, score=0.636, total=   0.0s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV] ......... C=1000, gamma=1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV] ......... C=1000, gamma=1, kernel=rbf, score=0.647, total=   0.0s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV] ......... C=1000, gamma=1, kernel=rbf, score=0.735, total=   0.0s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV] ......... C=1000, gamma=1, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV] ......... C=1000, gamma=1, kernel=rbf, score=0.667, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] ....... C=1000, gamma=0.1, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] ....... C=1000, gamma=0.1, kernel=rbf, score=0.794, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] ....... C=1000, gamma=0.1, kernel=rbf, score=0.559, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] ....... C=1000, gamma=0.1, kernel=rbf, score=0.788, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] ....... C=1000, gamma=0.1, kernel=rbf, score=0.485, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV] ...... C=1000, gamma=0.01, kernel=rbf, score=0.853, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV] ...... C=1000, gamma=0.01, kernel=rbf, score=0.794, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV] ...... C=1000, gamma=0.01, kernel=rbf, score=0.706, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV] ...... C=1000, gamma=0.01, kernel=rbf, score=0.788, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV] ...... C=1000, gamma=0.01, kernel=rbf, score=0.545, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] ..... C=1000, gamma=0.001, kernel=rbf, score=0.794, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] ..... C=1000, gamma=0.001, kernel=rbf, score=0.824, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] ..... C=1000, gamma=0.001, kernel=rbf, score=0.676, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] ..... C=1000, gamma=0.001, kernel=rbf, score=0.879, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] ..... C=1000, gamma=0.001, kernel=rbf, score=0.606, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] .... C=1000, gamma=0.0001, kernel=rbf, score=0.853, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] .... C=1000, gamma=0.0001, kernel=rbf, score=0.882, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] .... C=1000, gamma=0.0001, kernel=rbf, score=0.706, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] .... C=1000, gamma=0.0001, kernel=rbf, score=0.788, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] .... C=1000, gamma=0.0001, kernel=rbf, score=0.636, total=   0.0s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.87      0.80        47\n",
      "           1       0.62      0.40      0.49        25\n",
      "\n",
      "    accuracy                           0.71        72\n",
      "   macro avg       0.68      0.64      0.64        72\n",
      "weighted avg       0.69      0.71      0.69        72\n",
      "\n",
      "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 125 out of 125 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "model_svc = SVC(probability=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "                        X, Y, \n",
    "                test_size = 0.30, random_state = 101) \n",
    "\n",
    "model_svc.fit(X_train, y_train)\n",
    "\n",
    "preda=model_svc.predict(X_test)\n",
    "print(classification_report(y_test, preda))\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['rbf']}  \n",
    "  \n",
    "grid = GridSearchCV(SVC(probability=True), param_grid, refit = True, verbose = 3) \n",
    "  \n",
    "grid.fit(X_train, y_train) \n",
    "\n",
    "grid_predictions = grid.predict(X_test) \n",
    "\n",
    "print(classification_report(y_test, grid_predictions))  \n",
    "print(grid.best_params_) \n",
    "print(grid.best_estimator_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently hyperparameter tuning does not help in this case. Thus perform prediction using the old logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Team  Probability  Prediction\n",
      "0   ARI    31.150825           0\n",
      "1   ATL    17.553153           0\n",
      "2   BAL    53.163528           1\n",
      "3   BOS    12.661454           0\n",
      "4   CHC    83.040529           1\n",
      "5   CHW    17.058149           0\n",
      "6   CIN    43.136621           0\n",
      "7   CLE    11.326538           0\n",
      "8   COL    70.816525           1\n",
      "9   DET    13.061752           0\n",
      "10  HOU    60.793937           1\n",
      "11  KCR     3.868289           0\n",
      "12  LAA    50.827103           1\n",
      "13  LAD    57.108112           1\n",
      "14  MIA    27.250176           0\n",
      "15  MIL     4.160920           0\n",
      "16  MIN    30.270392           0\n",
      "17  NYM    77.672358           1\n",
      "18  NYY    87.175147           1\n",
      "19  OAK    47.563303           0\n",
      "20  PHI    90.666869           1\n",
      "21  PIT     1.140761           0\n",
      "22  SDP    24.507434           0\n",
      "23  SEA    10.759323           0\n",
      "24  SFG    14.734669           0\n",
      "25  STL     0.988297           0\n",
      "26  TBR    76.495594           1\n",
      "27  TEX     8.408854           0\n",
      "28  TOR     1.349251           0\n",
      "29  WSN     5.305377           0\n"
     ]
    }
   ],
   "source": [
    "DF_2020_1 = StandardScaler().fit_transform(DF_2020)\n",
    "predictions_2020_proba = model.predict_proba(DF_2020_1)\n",
    "predictions_2020_pred = model.predict(DF_2020_1)\n",
    "data_result = {'Team': df_2020['Tm'],\n",
    "        'Probability': predictions_2020_proba[:,1]*100,\n",
    "              'Prediction': predictions_2020_pred}\n",
    "prediction_table = pd.DataFrame(data_result)\n",
    "print(prediction_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order get more accurate result. I decided to adjust my model to only include the variables that are more significantly correlated to predicting the postseason birth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model from sklearn doesn't provide p-value automatically, so I turned to the logistic regression model from statsmodel to see which variables are less significantly correlated to predicting the postseason birth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.501195\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                   POST   No. Observations:                  240\n",
      "Model:                          Logit   Df Residuals:                      226\n",
      "Method:                           MLE   Df Model:                           13\n",
      "Date:                Wed, 16 Dec 2020   Pseudo R-squ.:                  0.2126\n",
      "Time:                        22:21:42   Log-Likelihood:                -120.29\n",
      "converged:                       True   LL-Null:                       -152.76\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.723e-09\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             1.3429      0.974      1.378      0.168      -0.567       3.252\n",
      "x2             3.2892      2.137      1.539      0.124      -0.900       7.478\n",
      "x3            -7.7348      5.924     -1.306      0.192     -19.346       3.877\n",
      "x4             0.0444      0.823      0.054      0.957      -1.568       1.657\n",
      "x5            -2.9628      2.257     -1.313      0.189      -7.386       1.461\n",
      "x6            -0.0494      0.209     -0.237      0.813      -0.459       0.360\n",
      "x7            -0.3901      0.230     -1.694      0.090      -0.842       0.061\n",
      "x8            -0.3932      0.764     -0.515      0.607      -1.890       1.104\n",
      "x9            -0.2961      0.239     -1.241      0.215      -0.764       0.172\n",
      "x10            6.4508      4.922      1.311      0.190      -3.195      16.097\n",
      "x11           -5.2292      4.448     -1.176      0.240     -13.947       3.489\n",
      "x12          -17.1446     10.577     -1.621      0.105     -37.875       3.586\n",
      "x13           17.7719     12.821      1.386      0.166      -7.356      42.900\n",
      "x14            4.6839      8.959      0.523      0.601     -12.875      22.243\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm \n",
    "log_reg = sm.Logit(Y, X).fit() \n",
    "print(log_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the p-value, I decided to use only those p-value under 0.18, which is PA, R, CS, SLG, OPS. A surprising discovery here is that HR(p=0.957) is wildly non-siginificantly correlated with postseason birth, which is a bit the contrary of what teams pursue recently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = df_com[['PA', 'R', 'CS', 'SLG', 'OPS']]\n",
    "Y2 = df_com['POST']\n",
    "X2 = StandardScaler().fit_transform(X2)\n",
    "\n",
    "model2 = LogisticRegression(multi_class='multinomial')\n",
    "model2.fit(X2, Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Team  Probability  Prediction\n",
      "0   ARI    30.528836           0\n",
      "1   ATL    34.945587           0\n",
      "2   BAL    50.611437           1\n",
      "3   BOS    24.343055           0\n",
      "4   CHC    79.979493           1\n",
      "5   CHW    39.400260           0\n",
      "6   CIN    22.450180           0\n",
      "7   CLE     6.953635           0\n",
      "8   COL    81.358858           1\n",
      "9   DET    26.703734           0\n",
      "10  HOU    67.514209           1\n",
      "11  KCR     7.929352           0\n",
      "12  LAA    51.155164           1\n",
      "13  LAD    56.421238           1\n",
      "14  MIA    31.429079           0\n",
      "15  MIL     9.461973           0\n",
      "16  MIN    38.962639           0\n",
      "17  NYM    71.531353           1\n",
      "18  NYY    63.583383           1\n",
      "19  OAK    46.468942           0\n",
      "20  PHI    64.933070           1\n",
      "21  PIT     3.358363           0\n",
      "22  SDP    22.172423           0\n",
      "23  SEA    14.899682           0\n",
      "24  SFG    18.638615           0\n",
      "25  STL     1.257867           0\n",
      "26  TBR    71.328421           1\n",
      "27  TEX     5.664947           0\n",
      "28  TOR     3.660027           0\n",
      "29  WSN     2.925560           0\n"
     ]
    }
   ],
   "source": [
    "DF_2020_2 = DF_2020[['PA', 'R', 'CS', 'SLG', 'OPS']]\n",
    "DF_2020_2 = StandardScaler().fit_transform(DF_2020_2)\n",
    "predictions_2020_proba2 = model2.predict_proba(DF_2020_2)\n",
    "predictions_2020_pred2 = model2.predict(DF_2020_2)\n",
    "data_result2 = {'Team': df_2020['Tm'],\n",
    "        'Probability': predictions_2020_proba2[:,1]*100,\n",
    "              'Prediction': predictions_2020_pred2}\n",
    "prediction_table2 = pd.DataFrame(data_result2)\n",
    "print(prediction_table2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of teams were the same but there were slight differences for the probability value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Conclusion\n",
    "\n",
    "The result was quite satisfying given the list of teams consumes most of the powerhouse of MLB, but we still have to keep in mind that this research only considered the batting part of stats and the stats on 8/13. In my last prediction model it only considered 'PA', 'R', 'CS', 'SLG', 'OPS', which may give us a look at what matters most in teams probability of getting into postseason.\n",
    "\n",
    "Prediction using classifiers other than Logistic Regression can be found here: https://github.com/Allen-Ho-0302/2020PostseasonPrediction-DeepLearning_XGBoost_ClassificationTree_LogisticRegression. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
